{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "DJOSHI\n",
        "Devavrat Joshi\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "5LY8ENDz6UdM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "6c384049-f66a-4271-a0e1-15f43468bbd5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nDJOSHI\\nDevavrat Joshi\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Z_W9l6I_t6cr"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "    Block for definining common Program Parameters & Constants Definitions\n",
        "'''\n",
        "\n",
        "CLICKBAITS_PATH = \"\"\n",
        "REMOVE_PUNCTUATIONS = False\n",
        "REMOVE_DEMOJI = True\n",
        "TOP_N_ROUGE_SCORES = 8\n",
        "\n",
        "PERFORM_SENTENCE_EXTRACTION = True\n",
        "PERFORM_DOCUMENT_QA = True\n",
        "PERFORM_SENTENCE_QA = True\n",
        "\n",
        "USE_BEST_CONFIGURED_SETTINGS = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ewhxz-iVt9ag"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Sentence Extraction (SE) Constant Definitions Block\n",
        "\"\"\"\n",
        "\n",
        "SE_BERT_SEQSEQ_LABEL_CLASSIFCATION = False\n",
        "SE_MODELS = []\n",
        "\n",
        "if SE_BERT_SEQSEQ_LABEL_CLASSIFCATION == True:\n",
        "  SE_MODELS.append({\"name\":\"bert-base-uncased\", \"features_dim\": 384, \"pooling\": \"mean\"})\n",
        "else:\n",
        "  SE_MODELS.append({\"name\":\"sentence-transformers/paraphrase-MiniLM-L3-v2\", \"features_dim\": 384, \"pooling\": \"mean\"})\n",
        "  SE_HIDDEN_DIM = 1024\n",
        "\n",
        "# 0 means dont use, 1 means calculate based on training data and [1,4] or anything else use provided classweights array  \n",
        "SE_USE_CLASSWEIGHTS = 1\n",
        "\n",
        "SE_BALANCE_NEG_MULTIPLIER = 0\n",
        "SE_OPTIMIZER_NAME = \"AdamW\"\n",
        "SE_LOSS_FUNCTION_NAME = \"BCELoss\"\n",
        "\n",
        "SE_BERT_NUM_HIDDEN_LAYERS = 3\n",
        "SE_BERT_NUM_ATTENTION_HEADS = 12\n",
        "SE_DROPOUT_HIDDEN = 0.1\n",
        "SE_DROPOUT_ATTENTION = 0.1\n",
        "SE_MAX_LEN = 512\n",
        "SE_BATCH_SIZE = 8\n",
        "SE_EPOCH_MAX = 20\n",
        "SE_DROPOUT =0.5\n",
        "SE_LEARNING_RATE = 5e-5\n",
        "SE_WEIGHT_DECAY  = 5e-05\n",
        "SE_THRESHOLD = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2MqYyA-Tr1-M"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "    Question Answering (QA) Block for definining Program Parameters & Constants Definitions\n",
        "'''\n",
        "\n",
        "#Question Answering (QA) Parameters\n",
        "QA_MODELS = []\n",
        "#QA_MODELS.append({'name':'bert-base-uncased'})\n",
        "QA_MODELS.append({'name':'roberta-base'})\n",
        "#QA_MODELS.append({'name':'bert-large-uncased'})\n",
        "\n",
        "\n",
        "QA_TRAIN_BATCH_SIZE = 16\n",
        "QA_VAL_BATCH_SIZE = 8\n",
        "\n",
        "QA_BERT_NUM_HIDDEN_LAYERS = 12\n",
        "QA_BERT_NUM_ATTENTION_HEADS = 12\n",
        "QA_OPTIMIZER_NAME = \"AdamW\"\n",
        "QA_LEARNING_RATE_SCHEDULER = False\n",
        "QA_DROPOUT_HIDDEN = 0.4\n",
        "QA_DROPOUT_ATTENTION = 0.4\n",
        "QA_LEARNING_RATE = 3e-5\n",
        "QA_WEIGHT_DECAY = 0.01\n",
        "QA_EPOCH_MAX=30\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Override above settings for best configuration\n",
        "'''\n",
        "def set_se_configuration():\n",
        "  if USE_BEST_CONFIGURED_SETTINGS == True:\n",
        "      global TOP_N_ROUGE_SCORES\n",
        "      global SE_BERT_SEQSEQ_LABEL_CLASSIFCATION\n",
        "      global SE_DROPOUT_HIDDEN\n",
        "      global SE_DROPOUT_ATTENTION\n",
        "      global SE_DROPOUT\n",
        "      global SE_LEARNING_RATE\n",
        "      global SE_WEIGHT_DECAY\n",
        "      global SE_EPOCH_MAX\n",
        "      global SE_USE_CLASSWEIGHTS\n",
        "\n",
        "      SE_USE_CLASSWEIGHTS = 0\n",
        "      TOP_N_ROUGE_SCORES = 8\n",
        "      SE_BERT_SEQSEQ_LABEL_CLASSIFCATION = False\n",
        "      SE_DROPOUT_HIDDEN = 0.1\n",
        "      SE_DROPOUT_ATTENTION = 0.1\n",
        "      SE_DROPOUT =0.5\n",
        "      SE_LEARNING_RATE = 5e-5\n",
        "      SE_WEIGHT_DECAY  = 5e-05\n",
        "      SE_EPOCH_MAX = 10  \n",
        "        \n",
        "def set_qa_document_configuration():\n",
        "  if USE_BEST_CONFIGURED_SETTINGS == True:\n",
        "      global TOP_N_ROUGE_SCORES\n",
        "      global QA_DROPOUT_HIDDEN\n",
        "      global QA_DROPOUT_ATTENTION\n",
        "      global QA_LEARNING_RATE\n",
        "      global QA_WEIGHT_DECAY\n",
        "      global QA_EPOCH_MAX\n",
        "      global QA_MODELS\n",
        "\n",
        "      TOP_N_ROUGE_SCORES = 0\n",
        "      QA_DROPOUT_HIDDEN = 0.2\n",
        "      QA_DROPOUT_ATTENTION = 0.2\n",
        "      QA_LEARNING_RATE = 3e-5\n",
        "      QA_WEIGHT_DECAY = 0.01\n",
        "      QA_EPOCH_MAX=10\n",
        "      QA_MODELS = []\n",
        "      QA_MODELS.append({'name':'roberta-base'})\n",
        "\n",
        "        \n",
        "def set_qa_statement_configuration():\n",
        "  if USE_BEST_CONFIGURED_SETTINGS == True:\n",
        "      global TOP_N_ROUGE_SCORES\n",
        "      global QA_DROPOUT_HIDDEN\n",
        "      global QA_DROPOUT_ATTENTION\n",
        "      global QA_LEARNING_RATE\n",
        "      global QA_WEIGHT_DECAY\n",
        "      global QA_EPOCH_MAX\n",
        "      global QA_MODELS\n",
        "      \n",
        "      TOP_N_ROUGE_SCORES = 0\n",
        "      QA_DROPOUT_HIDDEN = 0.3\n",
        "      QA_DROPOUT_ATTENTION = 0.3\n",
        "      QA_LEARNING_RATE = 3e-5\n",
        "      QA_WEIGHT_DECAY = 0.01\n",
        "      QA_EPOCH_MAX=12\n",
        "      QA_MODELS = []\n",
        "      QA_MODELS.append({'name':'roberta-base'})"
      ],
      "metadata": {
        "id": "CTqw5SnA3ceS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTsR_E_YsIk7",
        "outputId": "ca5e2da4-9751-48f4-f3ae-1ca9e11a94bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 7.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting demoji\n",
            "  Downloading demoji-1.1.0-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: demoji\n",
            "Successfully installed demoji-1.1.0\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "    Block for loading training data and creating dataset and data loaders\n",
        "'''\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from string import punctuation\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "from pandas.core.common import SettingWithCopyWarning\n",
        "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
        "\n",
        "\n",
        "try:\n",
        "  from unidecode import unidecode\n",
        "except ModuleNotFoundError:\n",
        "  !pip install unidecode\n",
        "  from unidecode import unidecode\n",
        "  \n",
        "try:\n",
        "  import operator\n",
        "except ModuleNotFoundError:\n",
        "  !pip install operator\n",
        "\n",
        "try:\n",
        "  from rouge import Rouge\n",
        "except ModuleNotFoundError:\n",
        "  !pip install rouge\n",
        "  from rouge import Rouge\n",
        "\n",
        "!pip install demoji\n",
        "import demoji\n",
        "\n",
        "demoji.download_codes()\n",
        "rouge_score = Rouge()\n",
        "\n",
        "def getby_uuid(df, uuid):\n",
        "  return df.loc[df['uuid'] == uuid]\n",
        "\n",
        "\"\"\" In some cases the spoilerPositions has defined targetParagraph index as -1.\n",
        "  I have observed in such cases the spoiler text is extracted from targetTitle.\n",
        "  Hence, i am appending targetTitle to targetParagraphs. -1 index will automatically take the last appended element from the tgp list\n",
        "\"\"\"\n",
        "def append_targetTitle_to_targetParagraphs(record):\n",
        "  targetParagraphs = record['targetParagraphs']\n",
        "  \n",
        "  append_title = False\n",
        "  for spoilerPosition in record['spoilerPositions']:\n",
        "      tgpStartIndex = spoilerPosition[0][0]\n",
        "      if tgpStartIndex == -1:\n",
        "          append_title = True\n",
        "          break\n",
        "      \n",
        "      if len(spoilerPosition) == 2 :\n",
        "          tgpEndIndex = spoilerPosition[1][0]\n",
        "          if tgpEndIndex == -1:\n",
        "              append_title = True\n",
        "              break\n",
        "\n",
        "  if append_title == True:\n",
        "      targetParagraphs.append(record['targetTitle'])\n",
        "  return targetParagraphs\n",
        "\n",
        "\"\"\"\n",
        "  targetParagraphs is used as single source for creating document and spoiler. Hence targetParagraphs needs to be curated. \n",
        "  Curation involves\n",
        "    1. replacing special character quotes with regualar quotes\n",
        "    2. trim trailing and leading whitespaces\n",
        "    3. convert to lower case\n",
        "\"\"\"\n",
        "def curate_text_array(texts):\n",
        "  textCurated = []\n",
        "  for text in texts:\n",
        "    text = curate_text(text)\n",
        "    textCurated.append(text)\n",
        "  \n",
        "  return textCurated\n",
        "\n",
        "def curate_text(text):\n",
        "    text = demoji_text(text)\n",
        "    text = unidecode(text)\n",
        "    text = text.replace('’','\\'') # data contains special character quotes which messes up tokenization\n",
        "    if REMOVE_PUNCTUATIONS == True:\n",
        "        punct_removed_tokens = []\n",
        "        tokens = text.split()\n",
        "        for token in tokens:\n",
        "            token = token.strip()\n",
        "            token = token.strip(punctuation)\n",
        "            if len(token) > 0:\n",
        "                punct_removed_tokens.append(token)\n",
        "        text = \" \".join(punct_removed_tokens)\n",
        "        \n",
        "    text = text.strip().lower()\n",
        "    return text\n",
        "\n",
        "def demoji_text(text):\n",
        "  if REMOVE_DEMOJI == True:\n",
        "    dem = demoji.findall(text)\n",
        "    for item in dem.keys():\n",
        "        text = text.replace(item, '')\n",
        "  return text\n",
        "\n",
        "\"\"\"\n",
        "Create document by concatenating targetParagraphs (tgp)\n",
        "This document and each targetParagraph is fed as an input pair to the sentence extraction model\n",
        "\"\"\"\n",
        "def create_tgp_document(record):\n",
        "    topn_targetParagraphs = []\n",
        "    \n",
        "    if TOP_N_ROUGE_SCORES > 0:\n",
        "        scores = record['tgpRougeScore']\n",
        "        topn_max  = len(scores) if len(scores) < TOP_N_ROUGE_SCORES else TOP_N_ROUGE_SCORES\n",
        "        topn_scores = scores[0:topn_max]\n",
        "        topn_indexes = [score['index'] for score in topn_scores]\n",
        "        topn_indexes.sort()\n",
        "    \n",
        "        \n",
        "        for topn_index in topn_indexes:\n",
        "            tgp = record['targetParagraphs'][topn_index]\n",
        "            topn_targetParagraphs.append(tgp)\n",
        "    else:\n",
        "        topn_targetParagraphs = record['targetParagraphs']\n",
        "    \n",
        "    return \" \".join(topn_targetParagraphs)\n",
        "\n",
        "def string_list_to_string(str_lst):\n",
        "    return str_lst[0]\n",
        "\n",
        "# Mark each targetParagraph with 0 (Not to be extracted) and 1 (to be extracted) label depending on definition in spoilerPosition\n",
        "def mark_tgp_extraction_label(record):\n",
        "    tgpSELabels = [0] * record['tgpsCount']\n",
        "    \n",
        "    for spoilerPosition in record['spoilerPositions']:\n",
        "        tgpSELabels[spoilerPosition[0][0]] = 1\n",
        "        if len(spoilerPosition) == 2:\n",
        "            tgpSELabels[spoilerPosition[1][0]] = 1\n",
        "    return tgpSELabels\n",
        "\n",
        "def calc_rouge_score(record):\n",
        "    scores = [{}] * record['tgpsCount']\n",
        "    for i in range(record['tgpsCount']):\n",
        "        try:\n",
        "            score = rouge_score.get_scores(record['postText'], record['targetParagraphs'][i])\n",
        "            scores[i] = {\"index\":i, \"score\": score[0]['rouge-l']['p']}\n",
        "        except Exception as ex:\n",
        "            scores[i] = {\"index\":i, \"score\": 0}\n",
        "\n",
        "    scores.sort(key=operator.itemgetter('score'), reverse = True)\n",
        "    \n",
        "    return scores\n",
        "\n",
        "def assing_tgpId(record):\n",
        "    uuid = record['uuid']\n",
        "    tgpIds = []\n",
        "    for i in range(len(record['targetParagraphs'])):\n",
        "      tgpId = (uuid, i)\n",
        "      tgpIds.append(tgpId)\n",
        "    return tgpIds\n",
        "\n",
        "def append_from_source(record, val_df):\n",
        "  tgpId = record['tgpId']\n",
        "  uuid = tgpId[0]\n",
        "  statementId = tgpId[1]\n",
        "  val_df_record = getby_uuid(val_df, uuid)\n",
        "  val_df_record.reset_index(drop=True,inplace=True)\n",
        "  postTexts = val_df_record['postText'][0]\n",
        "  targetParagraphs = val_df_record['targetParagraphs'][0][statementId]\n",
        "  tgpDocuments = val_df_record['tgpDocument'][0]\n",
        "  tgpQALabels = val_df_record['tgpQALabel'][0][statementId]\n",
        "  spoiler = val_df_record['spoiler'][0]\n",
        "  spoilerPositions = val_df_record['spoilerPositions'][0]\n",
        "  tgpsCount = val_df_record['tgpsCount'][0]\n",
        "  return pd.Series([postTexts, targetParagraphs, tgpDocuments, spoiler, spoilerPositions, tgpsCount, tgpQALabels])\n",
        "\n",
        "def append_info_to_se_output(val_df, se_outputs_df):\n",
        "  \n",
        "  se_outputs_df[['postText','targetParagraphs','tgpDocument','spoiler','spoilerPositions','tgpsCount', 'tgpQALabel']] = se_outputs_df.apply(append_from_source, val_df=val_df, axis=1)\n",
        "  se_outputs_df.reset_index(level=0, inplace=True)\n",
        "  se_outputs_df.to_csv('output_se.csv')\n",
        "\n",
        "def load_data(filename):\n",
        "    df = pd.read_json(os.path.join(CLICKBAITS_PATH, filename), lines=True)\n",
        "    \n",
        "    df['tags'] = df['tags'].apply(string_list_to_string)\n",
        "    df['targetParagraphs'] = df.apply(append_targetTitle_to_targetParagraphs, axis=1)\n",
        "    df['targetParagraphs'] = df['targetParagraphs'].apply(curate_text_array)\n",
        "    df['tgpId'] = df.apply(assing_tgpId, axis = 1)\n",
        "    df['postText'] = df['postText'].apply(curate_text_array)\n",
        "    df['postText'] = df['postText'].apply(string_list_to_string)\n",
        "    df['spoiler'] = df['spoiler'].apply(curate_text_array)\n",
        "    df['tgpsCount'] = df['targetParagraphs'].apply(len)\n",
        "    \n",
        "    df['tgpSELabel'] = df.apply(mark_tgp_extraction_label, axis=1)\n",
        "    df['tgpRougeScore'] = df.apply(calc_rouge_score, axis =1)\n",
        "\n",
        "    df['tgpDocument'] = df.apply(create_tgp_document, axis = 1)\n",
        "\n",
        "    df.reset_index(level=0, inplace=True)\n",
        "    phrase_passage_df = df[df.tags != 'multi']\n",
        "    multi_df = df[df.tags == 'multi']\n",
        "\n",
        "    return df, phrase_passage_df, multi_df\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fJaCW9mgvLVO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b24b49e5-7773-4c95-df73-2792a052d60f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 6.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 90.3 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 77.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "  Sentence Extraction \n",
        "    Sentence Extraction Block for  :\n",
        "        1.  Dataset and Dataloader cell \n",
        "        2.  SentenceBertClass Model Definition\n",
        "        2.  factory method of model creation\n",
        "        3.  train loop - train_epoch and validate_epoch\n",
        "'''\n",
        "from torch.utils.data import Dataset, DataLoader \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim \n",
        "from sklearn.metrics import f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm_notebook\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "try:\n",
        "  from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
        "except ModuleNotFoundError:\n",
        "  !pip install transformers\n",
        "  from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
        "\n",
        "# Dataset for ClickBaitsWebis22 (CBW22) Sentence Extraction (SE)\n",
        "class CBW22SEDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        sentence = str(self.data.iloc[index].targetParagraphs)\n",
        "        document = str(self.data.iloc[index].postText)\n",
        "        tgpId = self.data.iloc[index].tgpId\n",
        "\n",
        "        inputs = self.tokenizer.batch_encode_plus(\n",
        "            [sentence, document], \n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding=\"max_length\",\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "\n",
        "        return {\n",
        "            'tgpId': tgpId,\n",
        "            'sent_ids': torch.tensor(ids[0], dtype=torch.long),\n",
        "            'doc_ids': torch.tensor(ids[1], dtype=torch.long),\n",
        "            'sent_mask': torch.tensor(mask[0], dtype=torch.long),\n",
        "            'doc_mask': torch.tensor(mask[1], dtype=torch.long),\n",
        "            'targets': torch.tensor([self.data.iloc[index].tgpSELabel], dtype=torch.long)\n",
        "        } \n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "def balance_data(df):\n",
        "\n",
        "  print(\"Pre-balancing dataset size: {}\".format(len(df)))\n",
        "\n",
        "  pos_df = df[df.tgpSELabel == 1]\n",
        "  neg_df = df[df.tgpSELabel == 0]\n",
        "\n",
        "  print(\"Negative sample size:\", len(neg_df))\n",
        "  print(\"Positive sample size:\", len(pos_df))\n",
        "\n",
        "  sub_neg_df = neg_df.sample(len(pos_df)*SE_BALANCE_NEG_MULTIPLIER) \n",
        "  balanced_df = pos_df.append(sub_neg_df)\n",
        "\n",
        "  print(\"Balanced dataset size: {}\".format(len(balanced_df)))\n",
        "  \n",
        "  return balanced_df\n",
        "\n",
        "def get_data_se(tokenizer, df, balance = False, cal_class_weights = False):\n",
        "  classWeightsTensor = None\n",
        "\n",
        "  if balance == True:\n",
        "    df = balance_data(df)\n",
        "  \n",
        "  df['tgpQALabel'] = df.apply(compute_qa_answer_label_statement, axis = 1)\n",
        "  df['topNRougeStatement'] = df.apply(mark_rouge_topn_tgp_for_sentence_qa, axis = 1)\n",
        "  \n",
        "  col_list = ['tgpId','postText', 'targetParagraphs', 'tgpDocument', 'tgpSELabel', 'tgpQALabel', 'topNRougeStatement']\n",
        "  df = df[col_list]\n",
        "  \n",
        "  df = df.apply(pd.Series.explode)\n",
        "  # get only those targetParagraphs which appear in spoiler\n",
        "  df = df[df.topNRougeStatement == True]\n",
        "\n",
        "  if cal_class_weights == True:\n",
        "    classWeightsTensor = getClassWeights(df)\n",
        "\n",
        "  ds = CBW22SEDataset(df, tokenizer, SE_MAX_LEN)\n",
        "  dl = DataLoader(batch_size=SE_BATCH_SIZE, shuffle=True, dataset=ds)\n",
        "  return dl, classWeightsTensor\n",
        "\n",
        "\n",
        "# calculate weights of lables as their imbalance in distribution\n",
        "def getClassWeights(df):\n",
        "  classWeightsTensor = None\n",
        "  if SE_USE_CLASSWEIGHTS != 0:\n",
        "    if SE_USE_CLASSWEIGHTS == 1:\n",
        "        Y = df['tgpSELabel'].explode()\n",
        "        uniqueY = df.tgpSELabel.explode().unique()\n",
        "        classWeights = class_weight.compute_class_weight(class_weight=\"balanced\", classes=uniqueY, y=Y)\n",
        "    else:\n",
        "        classWeights = SE_USE_CLASSWEIGHTS\n",
        "    print(\"Using class Weights: {}\".format(classWeights))    \n",
        "    classWeightsTensor = torch.tensor(classWeights, dtype=torch.float32).to(device)\n",
        "  return classWeightsTensor\n",
        "\n",
        "# get mean pooling for sentence bert models \n",
        "# ref https://www.sbert.net/examples/applications/computing-embeddings/README.html#sentence-embeddings-with-transformers\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
        "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "    return sum_embeddings / sum_mask\n",
        "\n",
        "def max_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    token_embeddings[input_mask_expanded == 0] = -1e9  # Set padding tokens to large negative value\n",
        "    return torch.max(token_embeddings, 1)[0]\n",
        "\n",
        "def cls_pooling(model_output, attention_mask):\n",
        "    return model_output[0][:,0]\n",
        "\n",
        "'''\n",
        "  customized model with a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
        "'''\n",
        "class SentenceBertClass(torch.nn.Module):\n",
        "    def __init__(self, configuration, sentence_model_name, features_dim, pooling):\n",
        "        super(SentenceBertClass, self).__init__()\n",
        "        self.pooling = pooling\n",
        "        self.l1 = AutoModel.from_pretrained(pretrained_model_name_or_path = sentence_model_name, config = configuration)\n",
        "        self.pre_classifier = torch.nn.Linear(features_dim * 3, SE_HIDDEN_DIM)\n",
        "        self.dropout = torch.nn.Dropout(SE_DROPOUT)\n",
        "        self.classifier = torch.nn.Linear(SE_HIDDEN_DIM, 1)\n",
        "        self.classifierSigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, sent_ids, doc_ids, sent_mask, doc_mask):\n",
        "        sent_output = self.l1(input_ids=sent_ids, attention_mask=sent_mask) \n",
        "\n",
        "        sentence_embeddings = sent_output\n",
        "        if self.pooling == \"mean\":\n",
        "          sentence_embeddings = mean_pooling(sent_output, sent_mask) \n",
        "        elif self.pooling == \"cls\":\n",
        "          sentence_embeddings = cls_pooling(sent_output, sent_mask)\n",
        "        elif self.pooling == \"max\":\n",
        "          sentence_embeddings = max_pooling(sent_output, sent_mask)\n",
        "\n",
        "        doc_output = self.l1(input_ids=doc_ids, attention_mask=doc_mask) \n",
        "\n",
        "        doc_embeddings = doc_output\n",
        "        if self.pooling == \"mean\":\n",
        "          doc_embeddings = mean_pooling(doc_output, doc_mask)\n",
        "        elif self.pooling == \"cls\":\n",
        "          doc_embeddings = cls_pooling(doc_output, doc_mask)\n",
        "        elif self.pooling == \"max\":\n",
        "          doc_embeddings = max_pooling(doc_output, doc_mask)\n",
        "\n",
        "        # elementwise product of sentence embs and doc embs\n",
        "        combined_features = sentence_embeddings * doc_embeddings\n",
        "\n",
        "        # Concatenate input features and their elementwise product\n",
        "        concat_features = torch.cat((sentence_embeddings, doc_embeddings, combined_features), dim=1)   \n",
        "\n",
        "        pooler = self.pre_classifier(concat_features) \n",
        "        pooler = torch.nn.ReLU()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        output = self.classifier(pooler)\n",
        "        output = self.classifierSigmoid(output) \n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "def create_model_se(sentence_model_name, features_dim, pooling):\n",
        "    \n",
        "    tokenizer = AutoTokenizer.from_pretrained(sentence_model_name) \n",
        "    \n",
        "    configuration = AutoConfig.from_pretrained(sentence_model_name)\n",
        "    configuration.hidden_dropout_prob = SE_DROPOUT_HIDDEN\n",
        "    configuration.attention_probs_dropout_prob = SE_DROPOUT_ATTENTION\n",
        "    configuration.num_hidden_layers = SE_BERT_NUM_HIDDEN_LAYERS\n",
        "    configuration.num_attention_heads = SE_BERT_NUM_ATTENTION_HEADS\n",
        "\n",
        "    if SE_BERT_SEQSEQ_LABEL_CLASSIFCATION == True:\n",
        "      model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path = sentence_model_name, config = configuration)\n",
        "    else:\n",
        "      model = SentenceBertClass(configuration, sentence_model_name, features_dim, pooling)\n",
        "\n",
        "    model.to(device)\n",
        "    \n",
        "    if SE_LOSS_FUNCTION_NAME == \"BCELoss\":\n",
        "        if SE_USE_CLASSWEIGHTS == 0:\n",
        "          loss_function = nn.BCELoss().to(device)\n",
        "        else:\n",
        "          loss_function = nn.BCELoss(reduction='none').to(device)\n",
        "  \n",
        "    if SE_OPTIMIZER_NAME == \"SGD\":\n",
        "      optimizer = optim.SGD(model.parameters(), lr=SE_LEARNING_RATE, weight_decay=SE_WEIGHT_DECAY)\n",
        "    elif SE_OPTIMIZER_NAME == \"Adam\":\n",
        "      optimizer = optim.Adam(model.parameters(), lr=SE_LEARNING_RATE, weight_decay=SE_WEIGHT_DECAY)\n",
        "    elif SE_OPTIMIZER_NAME == \"AdamW\":\n",
        "      optimizer = optim.AdamW(model.parameters(), lr=SE_LEARNING_RATE, weight_decay=SE_WEIGHT_DECAY)\n",
        "    \n",
        "    return model, loss_function, optimizer, tokenizer\n",
        "\n",
        "def train_epoch_se(epoch, model, loss_function, optimizer, train_loader, classWeightsTensor):    \n",
        "    tr_loss = []\n",
        "    f1scores = []\n",
        "    \n",
        "    model.train()\n",
        "    pbar = tqdm_notebook(train_loader, desc=\"SE Training batch for epoch-{}\".format(epoch))\n",
        "    for _,data in enumerate(pbar):\n",
        "        sent_ids = data['sent_ids'].to(device, dtype = torch.long)\n",
        "        doc_ids = data['doc_ids'].to(device, dtype = torch.long)\n",
        "        sent_mask = data['sent_mask'].to(device, dtype = torch.long)\n",
        "        doc_mask = data['doc_mask'].to(device, dtype = torch.long) \n",
        "        targets = data['targets'].to(device, dtype = torch.float)  \n",
        "\n",
        "        outputs = model(sent_ids, doc_ids, sent_mask, doc_mask) \n",
        "        loss = loss_function(outputs, targets)\n",
        "        \n",
        "        if SE_USE_CLASSWEIGHTS != 0:\n",
        "            batch_class_weight = classWeightsTensor[targets.data.view(-1).long()].view_as(targets)\n",
        "            loss_class_weighted = loss * batch_class_weight\n",
        "            loss = loss_class_weighted.mean()\n",
        "            batch_loss = loss.item()\n",
        "        else:\n",
        "            batch_loss = loss.item()\n",
        "\n",
        "        tr_loss.append(batch_loss)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        targets = targets.cpu().numpy()\n",
        "        outputs = outputs.detach().cpu().numpy()\n",
        "        outputs_pred = [[0.]] * len(outputs)\n",
        "        for i in range(len(outputs)):\n",
        "            if outputs[i][0] > SE_THRESHOLD:\n",
        "                outputs_pred[i][0] = 1.\n",
        "                \n",
        "        batch_f1score = f1_score(targets, outputs_pred, average='micro') * 100\n",
        "        f1scores.append(batch_f1score)\n",
        "\n",
        "        pbar.set_postfix_str(\"Loss: {}, F1 Score:{}\".format(round(batch_loss,2), round(batch_f1score,2)))\n",
        "        \n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_f1score = 0\n",
        "    if len(tr_loss) > 0 :\n",
        "      epoch_loss = sum(tr_loss)/len(tr_loss)\n",
        "    \n",
        "    if len(f1scores) > 0:\n",
        "      epoch_f1score = sum(f1scores)/len(f1scores)\n",
        "    \n",
        "    return epoch_loss, epoch_f1score\n",
        "\n",
        "def validate_epoch_se(epoch, model, loss_function, optimizer, dl, classWeightsTensor):\n",
        "    losses = []\n",
        "    f1scores = []\n",
        "    val_outputs = []\n",
        "\n",
        "    model.eval()\n",
        "    pbar = tqdm_notebook(dl, desc=\"SE Validation batch for epoch-{}\".format(epoch))\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(pbar): \n",
        "            \n",
        "            tgpIds = data['tgpId']\n",
        "            sent_ids = data['sent_ids'].to(device, dtype = torch.long)\n",
        "            doc_ids = data['doc_ids'].to(device, dtype = torch.long)\n",
        "            sent_mask = data['sent_mask'].to(device, dtype = torch.long)\n",
        "            doc_mask = data['doc_mask'].to(device, dtype = torch.long) \n",
        "            targets = data['targets'].to(device, dtype = torch.float)  \n",
        "\n",
        "            outputs = model(sent_ids, doc_ids, sent_mask, doc_mask) \n",
        "            loss = loss_function(outputs, targets)\n",
        "            \n",
        "            if SE_USE_CLASSWEIGHTS != 0:\n",
        "                batch_class_weight = classWeightsTensor[targets.data.view(-1).long()].view_as(targets)\n",
        "                loss_class_weighted = loss * batch_class_weight\n",
        "                loss = loss_class_weighted.mean()\n",
        "                batch_loss = loss.item()\n",
        "            else:\n",
        "                batch_loss = loss.item()\n",
        "            \n",
        "            losses.append(batch_loss)\n",
        "            \n",
        "            targets = targets.cpu().numpy()\n",
        "            outputs = outputs.detach().cpu().numpy()\n",
        "            outputs_pred = [[0.]] * len(outputs)\n",
        "            for i in range(len(outputs)):\n",
        "                if outputs[i][0] > SE_THRESHOLD:\n",
        "                    outputs_pred[i][0] = 1.\n",
        "            \n",
        "            batch_f1score = f1_score(targets, outputs_pred, average='micro') * 100\n",
        "            f1scores.append(batch_f1score)\n",
        "\n",
        "            pbar.set_postfix_str(\"Loss: {}, F1 Score:{}\".format(round(batch_loss,2), round(batch_f1score,2)))\n",
        "\n",
        "            for index in range(len(tgpIds[0])):\n",
        "                val_outputs.append({\n",
        "                    \"tgpId\": (tgpIds[0][index],tgpIds[1][index].item()), \n",
        "                    \"tgpSELabel\": int(targets[index][0]), \n",
        "                    \"sePredLabel\": int(outputs_pred[index][0])\n",
        "                })\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_f1score = 0\n",
        "    if len(losses) > 0 :\n",
        "      epoch_loss = sum(losses)/len(losses)\n",
        "    \n",
        "    if len(f1scores) > 0:\n",
        "      epoch_f1score = sum(f1scores)/len(f1scores)\n",
        "    \n",
        "    return epoch_loss, epoch_f1score, val_outputs\n",
        "\n",
        "'''\n",
        "  Sentence Extraction using BERT Sequence Label Classification model\n",
        "'''\n",
        "\n",
        "def train_epoch_se_bertlc(epoch, model, loss_function, optimizer, train_loader, classWeightsTensor):    \n",
        "    tr_loss = []\n",
        "    nb_tr_examples = 0\n",
        "    n_correct = 0\n",
        "    f1scores = []\n",
        "    \n",
        "    model.train()\n",
        "    pbar = tqdm_notebook(train_loader, desc=\"Training batch for epoch-{}\".format(epoch))\n",
        "    for _,data in enumerate(pbar):\n",
        "        targets = data['labels']\n",
        "        \n",
        "        outputs = model(**data)\n",
        "        \n",
        "        loss = outputs.loss\n",
        "\n",
        "        if SE_USE_CLASSWEIGHTS != 0:\n",
        "            batch_class_weight = classWeightsTensor[targets.data.view(-1).long()].view_as(targets)\n",
        "            loss_class_weighted = loss * batch_class_weight\n",
        "            loss = loss_class_weighted.mean()\n",
        "            batch_loss = loss.item()\n",
        "        else:\n",
        "            batch_loss = loss.item()\n",
        "            \n",
        "        tr_loss.append(batch_loss)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        logits = outputs.logits\n",
        "        outputs_pred = (logits > SE_THRESHOLD)\n",
        "        batch_correct = torch.count_nonzero(targets == outputs_pred).item()\n",
        "        n_correct += batch_correct\n",
        "        nb_tr_examples+=targets.size(0)\n",
        "\n",
        "        batch_accuracy = (batch_correct * 100)/targets.size(0)\n",
        "        batch_f1score = f1_score(targets.cpu(), outputs_pred.cpu(), average='micro') * 100\n",
        "        f1scores.append(batch_f1score)\n",
        "\n",
        "        pbar.set_postfix_str(\"Loss: {}, F1 Score:{}, Accuracy:{}\".format(round(batch_loss,2), round(batch_f1score,2), round(batch_accuracy,2)))\n",
        "        \n",
        "\n",
        "    epoch_loss = sum(tr_loss)/len(tr_loss)\n",
        "    epoch_f1score = sum(f1scores)/len(f1scores)\n",
        "    epoch_accuracy = (n_correct*100)/nb_tr_examples\n",
        "    \n",
        "    return epoch_loss, epoch_f1score, epoch_accuracy\n",
        "\n",
        "def validate_se_bertlc(epoch, model, loss_function, optimizer, dl, classWeightsTensor):\n",
        "    losses = []\n",
        "    model.eval()\n",
        "    nb_tr_examples = 0\n",
        "    n_correct = 0\n",
        "    f1scores = []\n",
        "\n",
        "    pbar = tqdm_notebook(dl, desc=\"Validation batch for epoch-{}\".format(epoch))\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(pbar): \n",
        "            \n",
        "            targets= data['labels']\n",
        "            outputs = model(**data)\n",
        "            \n",
        "            loss = outputs.loss\n",
        "            \n",
        "            if SE_USE_CLASSWEIGHTS != 0:\n",
        "                batch_class_weight = classWeightsTensor[targets.data.view(-1).long()].view_as(targets)\n",
        "                loss_class_weighted = loss * batch_class_weight\n",
        "                loss = loss_class_weighted.mean()\n",
        "                batch_loss = loss.item()\n",
        "            else:\n",
        "                batch_loss = loss.item()\n",
        "                \n",
        "            losses.append(batch_loss)\n",
        "            \n",
        "            logits = outputs.logits\n",
        "            outputs_pred = (logits > SE_THRESHOLD)\n",
        "            batch_correct = torch.count_nonzero(targets == outputs_pred).item()\n",
        "            n_correct += batch_correct\n",
        "            nb_tr_examples+=targets.size(0)\n",
        "\n",
        "            batch_accuracy = (batch_correct * 100)/targets.size(0)\n",
        "\n",
        "            batch_f1score = f1_score(targets.cpu(), outputs_pred.cpu(), average='micro') * 100\n",
        "            f1scores.append(batch_f1score)\n",
        "\n",
        "            pbar.set_postfix_str(\"Loss: {}, F1 Score:{}, Accuracy:{}\".format(round(batch_loss,2), round(batch_f1score,2), round(batch_accuracy,2)))\n",
        "\n",
        "    epoch_loss = sum(losses)/len(losses)\n",
        "    epoch_f1score = sum(f1scores)/len(f1scores)\n",
        "    epoch_accuracy = (n_correct*100)/nb_tr_examples\n",
        "    \n",
        "    return epoch_loss, epoch_f1score, epoch_accuracy\n",
        "\n",
        "\n",
        "def train_se(model, loss_function, optimizer, tokenizer, train_loader, val_loader, val_df, classWeightsTensor):\n",
        "    train_losses = []\n",
        "    train_f1scores = []\n",
        "    val_losses = []\n",
        "    val_f1scores = []\n",
        "    epochs = np.arange(SE_EPOCH_MAX)\n",
        "    val_outputs = []\n",
        "\n",
        "    pbar = tqdm_notebook(range(SE_EPOCH_MAX), desc=\"Epoch\", position = 0)\n",
        "    for epoch in pbar:\n",
        "\n",
        "        if SE_BERT_SEQSEQ_LABEL_CLASSIFCATION == True:\n",
        "          train_epoch_loss, train_epoch_f1score, epoch_accuracy = train_epoch_se_bertlc(epoch, model, loss_function, optimizer, train_loader, classWeightsTensor)\n",
        "        else:\n",
        "          train_epoch_loss, train_epoch_f1score  = train_epoch_se(epoch, model, loss_function, optimizer, train_loader, classWeightsTensor)\n",
        "        \n",
        "        train_losses.append(train_epoch_loss)\n",
        "        train_f1scores.append(train_epoch_f1score)\n",
        "        \n",
        "        if SE_BERT_SEQSEQ_LABEL_CLASSIFCATION == True:\n",
        "          val_epoch_loss, val_epoch_f1score, epoch_accuracy = validate_se_bertlc(epoch, model, loss_function, optimizer, val_loader, classWeightsTensor)\n",
        "        else:\n",
        "          val_epoch_loss, val_epoch_f1score, val_outputs = validate_epoch_se(epoch, model, loss_function, optimizer, val_loader, classWeightsTensor)\n",
        "\n",
        "        val_losses.append(val_epoch_loss)\n",
        "        val_f1scores.append(val_epoch_f1score)\n",
        "\n",
        "        pbar.set_postfix_str(\"Training loss: {} / F1 Score: {}, Validation Loss: {} / F1 Score: {}\".format(\n",
        "                                round(train_epoch_loss,2), round(train_epoch_f1score,2), \n",
        "                                round(val_epoch_loss,2), round(val_epoch_f1score,2)))\n",
        "\n",
        "    scores = {}\n",
        "    scores['Error'] = 'None'\n",
        "    scores['Training Loss'] = round(train_losses[-1], 2)\n",
        "    scores['Training F1Score'] = round(train_f1scores[-1], 2)\n",
        "    scores['Validation Loss'] = round(val_losses[-1], 2)\n",
        "    scores['Validation F1Score'] = round(val_f1scores[-1], 2)\n",
        "\n",
        "    plt.plot(epochs, train_losses, color = 'blue')\n",
        "    plt.plot(epochs, val_losses, color = 'red')\n",
        "    plt.axhline(y=scores['Training Loss'], linestyle='dotted', color = 'blue')\n",
        "    plt.axhline(y=scores['Validation Loss'], linestyle='dotted', color = 'red')\n",
        "    plt.gca().legend(('Training Loss: {}'.format(scores['Training Loss']),\n",
        "                      'Validation Loss: {}'.format(scores['Validation Loss'])))\n",
        "    plt.title(\"Training-Validation Loss Curve - Sentence Extraction\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.savefig('epoch_trainval_losses_se.jpg', bbox_inches = 'tight')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    plt.plot(epochs, train_f1scores, color = 'blue')\n",
        "    plt.plot(epochs, val_f1scores, color = 'red')\n",
        "    plt.axhline(y=scores['Training F1Score'], linestyle='dotted', color = 'blue')\n",
        "    plt.axhline(y=scores['Validation F1Score'], linestyle='dotted', color = 'red')\n",
        "    plt.gca().legend(('Training F1Score: {}'.format(scores['Training F1Score']),\n",
        "                      'Validation F1Score: {}'.format(scores['Validation F1Score'])))\n",
        "    plt.title(\"Training-Validation F1Score Curve - Sentence Extraction\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"F1Score\")\n",
        "    plt.savefig('epoch_trainval_f1scores_se.jpg', bbox_inches = 'tight')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    val_outputs_df = pd.DataFrame.from_dict(val_outputs)\n",
        "\n",
        "\n",
        "    return scores, val_outputs_df\n",
        "\n",
        "def print_stats_se(sentence_model_name, features_dim, pooling, scores):\n",
        "    print(\"Sentence Extraction - Statistics\")\n",
        "    print(scores)\n",
        "    print(\"--------------------------------------------------------------------\")\n",
        "\n",
        "    stats = {}\n",
        "    stats['Sentence Model'] = sentence_model_name\n",
        "    stats['FEATURES_DIM']  = features_dim\n",
        "    stats['HIDDEN_DIM'] = SE_HIDDEN_DIM\n",
        "    stats['POOLING'] = pooling\n",
        "    stats['Optimizer'] = SE_OPTIMIZER_NAME\n",
        "    stats['Loss'] = SE_LOSS_FUNCTION_NAME\n",
        "    stats['Learning Rate'] = SE_LEARNING_RATE\n",
        "    stats['Weight Decay'] = SE_WEIGHT_DECAY\n",
        "    stats['DROPOUT'] = SE_DROPOUT\n",
        "    stats['THRESHOLD'] = SE_THRESHOLD\n",
        "    stats['epoch'] = SE_EPOCH_MAX\n",
        "    stats['SE_BALANCE_NEG_MULTIPLIER'] = SE_BALANCE_NEG_MULTIPLIER\n",
        "    stats['SE_BERT_NUM_ATTENTION_HEADS'] = SE_BERT_NUM_ATTENTION_HEADS\n",
        "    stats['SE_DROPOUT_HIDDEN'] = SE_DROPOUT_HIDDEN\n",
        "    stats['SE_DROPOUT_ATTENTION'] = SE_DROPOUT_ATTENTION\n",
        "\n",
        "    stats.update(scores)\n",
        "\n",
        "    # write model parameters to file\n",
        "    with open('score_se.csv', 'w') as csvfile:  \n",
        "        writer = csv.writer(csvfile)\n",
        "        for key, value in stats.items():\n",
        "           writer.writerow([key, value])\n",
        "\n",
        "    # Append model parameters to file for iterative experiments\n",
        "    with open('scores_se.csv', 'a', encoding='utf8', newline='') as csvfile:\n",
        "        fc = csv.DictWriter(csvfile, fieldnames=stats.keys(),)\n",
        "\n",
        "        if csvfile.tell() == 0:\n",
        "            fc.writeheader()\n",
        "\n",
        "        fc.writerow(stats)\n",
        "\n",
        "    return stats\n",
        "\n",
        "\n",
        "\n",
        "def process_se(train_df, val_df):\n",
        "  \n",
        "  scores_se = []\n",
        "  val_outputs_df = None\n",
        "  pbar = tqdm_notebook(range(len(SE_MODELS)), desc=\"SE Training using Huggingface Model\")\n",
        "  for i in pbar:\n",
        "    scores = {}\n",
        "    \n",
        "    sentence_model_name = SE_MODELS[i][\"name\"]\n",
        "    features_dim = SE_MODELS[i][\"features_dim\"]\n",
        "    pooling = SE_MODELS[i][\"pooling\"]\n",
        "\n",
        "    pbar.set_postfix_str(sentence_model_name)\n",
        "\n",
        "    try:\n",
        "      model, loss_function, optimizer, tokenizer = create_model_se(sentence_model_name, features_dim, pooling)\n",
        "      train_loader, classWeightsTensor = get_data_se(tokenizer, train_df, True if SE_BALANCE_NEG_MULTIPLIER > 0 else False, True)\n",
        "      val_loader, _ = get_data_se(tokenizer, val_df, False, False)\n",
        "      \n",
        "      scores, val_outputs_df = train_se(model, loss_function, optimizer, tokenizer, train_loader, val_loader, val_df, classWeightsTensor)\n",
        "\n",
        "      val_outputs_df.to_csv(\"se_bcs_pred_output.csv\")\n",
        "      \n",
        "    except Exception as ex:\n",
        "      scores[\"Error\"] = '{}'.format(ex)\n",
        "      print(scores[\"Error\"])\n",
        "\n",
        "    print_stats_se(sentence_model_name, features_dim, pooling, scores)\n",
        "\n",
        "  return scores_se, val_outputs_df\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2GV52lSQuoHR"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "  Question&Answering Dataset and Dataloader cell\n",
        "'''\n",
        "from torch.utils.data import Dataset, DataLoader \n",
        "\n",
        "\n",
        "# Dataset for ClickBaitsWebis22 (CBW22) Question Answering (QA)\n",
        "class CBW22QADataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch = {}\n",
        "        for key, val in self.encodings.items():\n",
        "            if type(val[idx]) == str or isinstance(val[idx],tuple):\n",
        "                batch[key] = val[idx]\n",
        "            else:\n",
        "                batch[key] = torch.tensor(val[idx]).to(device)\n",
        "        return batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "# get context and question encodings from the CBW22 data\n",
        "def get_encodings(tokenizer, df, document_context):\n",
        "    contexts = []\n",
        "    if document_context == True:\n",
        "      contexts = df['tgpDocument'].tolist()\n",
        "    else:\n",
        "      contexts = df['targetParagraphs'].tolist()\n",
        "    \n",
        "    questions = df['postText'].tolist()\n",
        "    tgpIds = df['tgpId'].tolist()\n",
        "    encodingIds = np.arange(len(df))\n",
        "    \n",
        "    encodings = tokenizer(contexts, questions, truncation=True, padding=True)\n",
        "    encodings.update({'encodingIds': encodingIds, 'tgpIds': tgpIds, \"contexts\":contexts, \"questions\": questions})\n",
        "    return encodings\n",
        "\n",
        "def add_answer_positions_to_encodings(tokenizer, encodings, df):\n",
        "    answers = df['tgpQALabel'].tolist()\n",
        "    \n",
        "    # initialize lists to contain the token indices of answer start/end\n",
        "    start_token_positions = []\n",
        "    end_token_positions = []\n",
        "    start_char_positions = []\n",
        "    end_char_positions = []\n",
        "    \n",
        "    no_spoilers_count = 0\n",
        "    truncated_count = 0\n",
        "    end_pos_shifted = 0\n",
        "    for i in range(len(answers)):\n",
        "        if len(answers[i]['spoiler']) > 0:\n",
        "            # append start/end token position using char_to_token method\n",
        "            start_token_positions.append(encodings.char_to_token(i, answers[i]['start_pos']))\n",
        "            end_token_positions.append(encodings.char_to_token(i, answers[i]['end_pos']-1))\n",
        "            \n",
        "            # if start position is None, the answer passage has been truncated\n",
        "            if start_token_positions[-1] is None:\n",
        "                truncated_count += 1\n",
        "                start_token_positions[-1] = tokenizer.model_max_length\n",
        "            \n",
        "            # end position cannot be found, char_to_token found space, so shift position until found\n",
        "            if end_token_positions[-1] is None:\n",
        "              end_pos_shifted +=1\n",
        "              shift = 1\n",
        "              while end_token_positions[-1] is None:\n",
        "                  end_token_positions[-1] = encodings.char_to_token(i, answers[i]['end_pos'] - shift)\n",
        "                  shift += 1\n",
        "                  \n",
        "            start_char_positions.append(answers[i]['start_pos'])\n",
        "            end_char_positions.append(answers[i]['end_pos'])\n",
        "        else:\n",
        "            start_char_positions.append(answers[i]['start_pos'])\n",
        "            end_char_positions.append(answers[i]['end_pos'])\n",
        "            start_token_positions.append(0)\n",
        "            end_token_positions.append(0)\n",
        "            \n",
        "            no_spoilers_count += 1\n",
        "        \n",
        "    #print(\"Number of No Spoilers found (TopN Rouge excluded): {}\".format(no_spoilers_count)) \n",
        "    #print(\"Number of truncated documents by tokenizer: {}\".format(truncated_count)) \n",
        "    #print(\"Number of endshifted spoilers by tokenizer: {}\".format(end_pos_shifted)) \n",
        "\n",
        "    # update our encodings object with the new token-based start/end positions\n",
        "    encodings.update({\n",
        "        'start_token_positions': start_token_positions, \n",
        "        'end_token_positions': end_token_positions,\n",
        "        'start_char_positions':start_char_positions, \n",
        "        'end_char_positions':end_char_positions\n",
        "    })\n",
        "\n",
        "def mark_rouge_topn_tgp_for_sentence_qa(record):\n",
        "    topn_tgp_marked = [False] * len(record['targetParagraphs'])\n",
        "    \n",
        "    if TOP_N_ROUGE_SCORES > 0:\n",
        "        scores = record['tgpRougeScore']\n",
        "        topn_max  = len(scores) if len(scores) < TOP_N_ROUGE_SCORES else TOP_N_ROUGE_SCORES\n",
        "        topn_scores = scores[0:topn_max]\n",
        "        topn_indexes = [score['index'] for score in topn_scores]\n",
        "        topn_indexes.sort()\n",
        "        \n",
        "        for topn_index in topn_indexes:\n",
        "            topn_tgp_marked[topn_index] = True\n",
        "    else:\n",
        "        topn_tgp_marked = [True] * len(record['targetParagraphs'])\n",
        "    \n",
        "    return topn_tgp_marked\n",
        "\n",
        "\"\"\"\n",
        "  the original dataset has spoilerPositions which define the answer location i.e targetParagraphIndex and character span in that targetParagraph\n",
        "  There is another field 'spoiler' which has extracted text as answer from the targetParagraph\n",
        "  However, i have noticed that there is sometimes (1087 times to be precise) discripancy between the spoiler text provided and text extracted from \n",
        "  targetParagraph by using the spoilerPosition information. This text spoiler is required to calculate the BLEU score.\n",
        "  Hence, i am disregarding the provided spoiler text and reconstructing it using the spoilerPositions \n",
        "\"\"\"\n",
        "def compute_qa_answer_label_statement(record):\n",
        "    answerLabels = [{}] * record['tgpsCount']\n",
        "    \n",
        "    for i in range(len(record['spoiler'])):\n",
        "        spoiler = record['spoiler'][i]\n",
        "        spoilerPosition = record['spoilerPositions'][i]\n",
        "        tgpSpoilerStartIndex = spoilerPosition[0][0] # 0th index defines targetParagraphs index\n",
        "        tgpSpoilerEndIndex = tgpSpoilerStartIndex\n",
        "        \n",
        "        if len(spoilerPosition) == 2:\n",
        "            tgpSpoilerEndIndex = spoilerPosition[1][0] # 0th     index defines targetParagraphs index\n",
        "       \n",
        "        if tgpSpoilerEndIndex == tgpSpoilerStartIndex:\n",
        "            if spoiler in record['targetParagraphs'][tgpSpoilerStartIndex]:\n",
        "                start_pos = record['targetParagraphs'][tgpSpoilerStartIndex].index(spoiler)\n",
        "                end_pos = start_pos + len(spoiler)\n",
        "                \n",
        "                answerLabels[tgpSpoilerStartIndex] = {\n",
        "                    \"spoiler\": record['targetParagraphs'][tgpSpoilerStartIndex][start_pos:end_pos],\n",
        "                    \"start_pos\": start_pos, \n",
        "                    \"end_pos\": end_pos\n",
        "                }\n",
        "        else:\n",
        "            \n",
        "            spoilerTokens = spoiler.split()\n",
        "            for i in range(len(spoilerTokens), -1, -1):\n",
        "                searchSpoiler = \" \".join(spoilerTokens[0:i])\n",
        "                if searchSpoiler in record['targetParagraphs'][tgpSpoilerStartIndex]:\n",
        "                    start_pos = record['targetParagraphs'][tgpSpoilerStartIndex].index(searchSpoiler)\n",
        "                    end_pos = len(record['targetParagraphs'][tgpSpoilerStartIndex])\n",
        "                    \n",
        "                    answerLabels[tgpSpoilerStartIndex] = {\n",
        "                        \"spoiler\": record['targetParagraphs'][tgpSpoilerStartIndex][start_pos:end_pos],\n",
        "                        \"start_pos\": start_pos, \n",
        "                        \"end_pos\": end_pos\n",
        "                    }   \n",
        "                    break\n",
        "                \n",
        "                \n",
        "            for i in range(0, len(spoilerTokens)):\n",
        "                searchSpoiler = \" \". join(spoilerTokens[i:len(spoilerTokens)])\n",
        "                if searchSpoiler in record['targetParagraphs'][tgpSpoilerEndIndex]:\n",
        "                    start_pos = 0\n",
        "                    end_pos = len(searchSpoiler)\n",
        "                    \n",
        "                    answerLabels[tgpSpoilerEndIndex] = {\n",
        "                        \"spoiler\": record['targetParagraphs'][tgpSpoilerEndIndex][start_pos:end_pos],\n",
        "                        \"start_pos\": start_pos, \n",
        "                        \"end_pos\": end_pos\n",
        "                    }   \n",
        "                    break\n",
        "       \n",
        "    return answerLabels\n",
        "\n",
        "def compute_qa_answer_label_document(record):\n",
        "    # phrase and passage has only 1 spoiler\n",
        "    spoilerPosition = record['spoilerPositions'][0]\n",
        "    \n",
        "    spoiler = record['spoiler'][0]\n",
        "    \n",
        "    start_pos = 0\n",
        "    end_pos = 0\n",
        "    tgpDocument = record['tgpDocument']\n",
        "    if spoiler in tgpDocument:\n",
        "        start_pos = tgpDocument.index(spoiler)\n",
        "        end_pos = start_pos + len(spoiler)\n",
        "    \n",
        "    answer = {\n",
        "        \"spoiler\": record['tgpDocument'][start_pos:end_pos],\n",
        "        \"start_pos\": start_pos, \n",
        "        \"end_pos\": end_pos\n",
        "    }   \n",
        "    return answer\n",
        "\n",
        "def mark_spoiler_exists_flag(answerLabel):\n",
        "    return \"spoiler\" in answerLabel.keys()\n",
        "\n",
        "def prepare_data_qa(tokenizer, df, batch_size, document_context):\n",
        "  col_list = ['tgpId', 'postText', 'targetParagraphs' ,'tgpDocument', 'tgpQALabel']\n",
        "  df = df[col_list]\n",
        "\n",
        "  encodings = get_encodings(tokenizer, df, document_context)\n",
        "\n",
        "  #append spoiler positions \n",
        "  add_answer_positions_to_encodings(tokenizer, encodings, df)\n",
        "\n",
        "  ds = CBW22QADataset(encodings)\n",
        "  dl = DataLoader(batch_size=batch_size, shuffle=True, dataset=ds)\n",
        "  return encodings, dl\n",
        "\n",
        "def reset_tgp_id(record):\n",
        "    return (record['uuid'],-1)\n",
        "\n",
        "def prepare_data_qa_document(tokenizer, df, batch_size):\n",
        "  qad_df = df.copy()\n",
        "  qad_df['tgpQALabel'] = qad_df.apply(compute_qa_answer_label_document, axis = 1)\n",
        "  qad_df['tgpId'] = qad_df.apply(reset_tgp_id, axis = 1)\n",
        "  return prepare_data_qa(tokenizer, qad_df, batch_size, True)\n",
        "  \n",
        "def get_dataloader_qa_statement(tokenizer, df, batch_size):\n",
        "    df = df[df.tgpSELabel == 1]\n",
        "    # get only those targetParagraphs which appear in spoiler\n",
        "    df['spoilerExists'] = df['tgpQALabel'].apply(mark_spoiler_exists_flag)\n",
        "    df = df[df.spoilerExists == True]\n",
        "    return prepare_data_qa(tokenizer, df, batch_size, False)\n",
        "  \n",
        "def prepare_data_qa_statement(tokenizer, df, batch_size):\n",
        "  df['tgpQALabel'] = df.apply(compute_qa_answer_label_statement, axis = 1)\n",
        "  col_list = ['tgpId','postText', 'targetParagraphs', 'tgpDocument', 'tgpSELabel', 'tgpQALabel']\n",
        "  df = df[col_list]\n",
        "  df.reset_index(level=0, inplace=True)\n",
        "  df = df.apply(pd.Series.explode)\n",
        "  return get_dataloader_qa_statement(tokenizer, df, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BOqa8z801kR4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48b738fd-1611-4dd8-cd03-67e2355d4166"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 8.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "    Question Answering Block for  :\n",
        "        1. factory method of model creation\n",
        "        2. train loop - train_epoch and validate_epoch\n",
        "        3. prediction\n",
        "'''\n",
        "from string import punctuation\n",
        "\n",
        "try:\n",
        "  from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AutoConfig\n",
        "except ModuleNotFoundError:\n",
        "  !pip install transformers\n",
        "  from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AutoConfig\n",
        "\n",
        "try:\n",
        "  import sentencepiece\n",
        "except ModuleNotFoundError:\n",
        "  !pip install sentencepiece\n",
        "  import sentencepiece\n",
        "\n",
        "try:\n",
        "  from nltk.translate import bleu\n",
        "except ModuleNotFoundError:\n",
        "  !pip install nltk\n",
        "  from nltk.translate import bleu\n",
        "\n",
        "import math\n",
        "from tqdm.notebook import tqdm_notebook\n",
        "import numpy as np\n",
        "from string import punctuation\n",
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim \n",
        "from transformers import AdamW\n",
        "from transformers import get_scheduler\n",
        "\n",
        "\n",
        "def create_model_qa(model_name, dl_len):\n",
        "    configuration = AutoConfig.from_pretrained(model_name)\n",
        "    configuration.hidden_dropout_prob = QA_DROPOUT_HIDDEN\n",
        "    configuration.attention_probs_dropout_prob = QA_DROPOUT_ATTENTION\n",
        "    configuration.num_hidden_layers = QA_BERT_NUM_HIDDEN_LAYERS\n",
        "    configuration.num_attention_heads = QA_BERT_NUM_ATTENTION_HEADS\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(pretrained_model_name_or_path = model_name, config = configuration)\n",
        "    \n",
        "    model.to(device)\n",
        "\n",
        "    if QA_OPTIMIZER_NAME == \"SGD\":\n",
        "      optimizer = optim.SGD(model.parameters(), lr = QA_LEARNING_RATE, weight_decay = QA_WEIGHT_DECAY)\n",
        "    elif QA_OPTIMIZER_NAME == \"Adam\":\n",
        "      optimizer = optim.Adam(model.parameters(), lr = QA_LEARNING_RATE, weight_decay = QA_WEIGHT_DECAY)\n",
        "    elif QA_OPTIMIZER_NAME == \"AdamW\":\n",
        "      optimizer = optim.AdamW(model.parameters(), lr = QA_LEARNING_RATE, weight_decay = QA_WEIGHT_DECAY)\n",
        "      #optimizer = AdamW(model.parameters(), lr=QA_LEARNING_RATE)\n",
        "\n",
        "    lr_scheduler = None\n",
        "    if QA_LEARNING_RATE_SCHEDULER == True:\n",
        "      num_training_steps = QA_EPOCH_MAX * dl_len\n",
        "      lr_scheduler = get_scheduler(name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "    \n",
        "    return model, optimizer, lr_scheduler\n",
        "\n",
        "def compute_scores_qa(outputs, batch, encodings):\n",
        "    encodingIds = batch['encodingIds'].detach().cpu().numpy()\n",
        "    contexts = batch['contexts']\n",
        "    \n",
        "    pred_outputs = {\n",
        "        \"tgpDocument\": contexts,\n",
        "        \"postText\": batch['questions'],\n",
        "        \"spoiler\": [],\n",
        "        \"predictions\": [],\n",
        "        \"bleuscore\" : [],\n",
        "        \"spoiler_punct_strip\": [],\n",
        "        \"predictions_punct_stip\": [],\n",
        "        \"bluescore_punct_strip\":[],\n",
        "        \"token span accuracy\": []\n",
        "    }\n",
        "\n",
        "    start_positions = batch['start_token_positions'].to(device)\n",
        "    end_positions = batch['end_token_positions'].to(device) \n",
        "    start_char_positions = batch['start_char_positions'].detach().cpu().numpy()\n",
        "    end_char_positions = batch['end_char_positions'].detach().cpu().numpy()\n",
        "\n",
        "    # pull prediction tensors out and argmax to get predicted tokens\n",
        "    start_pred = torch.argmax(outputs['start_logits'], dim=1)\n",
        "    end_pred = torch.argmax(outputs['end_logits'], dim=1)\n",
        "\n",
        "    start_pred  = start_pred.detach().cpu().numpy()\n",
        "    end_pred  = end_pred.detach().cpu().numpy()\n",
        "\n",
        "    for i in range(len(encodingIds)):\n",
        "      encodingIndex = encodingIds[i]\n",
        "      text = contexts[i]\n",
        "\n",
        "      # calculate accuracy for both and append to accuracy list\n",
        "      accuracy_start_token = 100 if (start_pred[i] == start_positions[i]) else 0 \n",
        "      accuracy_end_token = 100 if (end_pred[i] == end_positions[i]) else 0 \n",
        "      batch_accuracy_token = (accuracy_start_token + accuracy_end_token)/2\n",
        "      pred_outputs[\"token span accuracy\"].append(batch_accuracy_token)\n",
        "\n",
        "      pred_start_char_pos = 0\n",
        "      pred_end_char_pos = 0\n",
        "      \n",
        "      pos = encodings.token_to_chars(encodingIndex, start_pred[i])\n",
        "      if pos != None:\n",
        "          pred_start_char_pos = pos.start\n",
        "         \n",
        "      pos = encodings.token_to_chars(encodingIndex, end_pred[i])\n",
        "      if pos != None:\n",
        "          pred_end_char_pos = pos.end\n",
        "      \n",
        "      predict_answer = text[pred_start_char_pos:pred_end_char_pos].strip()\n",
        "      pred_outputs[\"predictions\"].append(predict_answer)\n",
        "      target_answer = text[start_char_positions[i]:end_char_positions[i]].strip()\n",
        "      pred_outputs[\"spoiler\"].append(target_answer)\n",
        "\n",
        "      bleu_score = bleu([predict_answer.split()], target_answer.split(), (1,),) * 100\n",
        "      pred_outputs[\"bleuscore\"].append(bleu_score)\n",
        "\n",
        "      predict_answer_punctstrip = predict_answer.strip(punctuation)\n",
        "      pred_outputs[\"predictions_punct_stip\"].append(predict_answer_punctstrip)\n",
        "      targets_punctstrip = target_answer.strip(punctuation)\n",
        "      pred_outputs[\"spoiler_punct_strip\"].append(targets_punctstrip)\n",
        "\n",
        "      bleu_score_punctstrip = bleu([predict_answer_punctstrip.split()], targets_punctstrip.split(), (1,),) * 100\n",
        "      pred_outputs[\"bluescore_punct_strip\"].append(bleu_score_punctstrip)\n",
        "    \n",
        "   \n",
        "    batch_accuracy_token = sum(pred_outputs[\"token span accuracy\"])/len(pred_outputs[\"token span accuracy\"])\n",
        "    batch_bleuscore = sum(pred_outputs[\"bleuscore\"])/len(pred_outputs[\"bleuscore\"])\n",
        "    batch_bleuscore_punctstrip = sum(pred_outputs[\"bluescore_punct_strip\"])/len(pred_outputs[\"bluescore_punct_strip\"])\n",
        "\n",
        "    return batch_accuracy_token, batch_bleuscore, batch_bleuscore_punctstrip, pred_outputs\n",
        "\n",
        "def train_epoch_qa(epoch, model, optimizer, lr_scheduler, train_loader, train_encodings):\n",
        "    epoch_loss = []\n",
        "    epoch_accuracies_token = []\n",
        "    epoch_bleu_scores = []\n",
        "    epoch_bleu_scores_punctstrip = []\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    pbar = tqdm_notebook(train_loader, desc=\"Training batch for epoch-{}\".format(epoch))\n",
        "    for _,batch in enumerate(pbar):\n",
        "        # initialize calculated gradients (from prev step)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # pull all the tensor batches required for training\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        start_positions = batch['start_token_positions'].to(device)\n",
        "        end_positions = batch['end_token_positions'].to(device)\n",
        "        \n",
        "        # train model on batch and return outputs (incl. loss)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask,\n",
        "                        start_positions=start_positions,\n",
        "                        end_positions=end_positions)\n",
        "        # extract loss\n",
        "        loss = outputs[0]\n",
        "        # calculate loss for every parameter that needs grad update\n",
        "        loss.backward()\n",
        "        # update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        if lr_scheduler != None:\n",
        "          lr_scheduler.step()\n",
        "        \n",
        "        batch_loss = loss.item()\n",
        "        if math.isnan(batch_loss) == False:\n",
        "          epoch_loss.append(batch_loss)\n",
        "        \n",
        "        batch_accuracy_token, batch_bleuscore, batch_bleuscore_punct, _ = compute_scores_qa(outputs, batch, train_encodings)\n",
        "        epoch_accuracies_token.append(batch_accuracy_token)\n",
        "        epoch_bleu_scores.append(batch_bleuscore)\n",
        "        epoch_bleu_scores_punctstrip.append(batch_bleuscore_punct)\n",
        "        pbar.set_postfix_str(\"Loss: {}, BleuScore:{}\".format(round(batch_loss,2), round(batch_bleuscore,2)))\n",
        "    \n",
        "    loss = 0\n",
        "    if len(epoch_loss) > 0 :\n",
        "      loss = sum(epoch_loss) / len(epoch_loss)\n",
        "    \n",
        "    accuracy_token = 0\n",
        "    if len(epoch_accuracies_token) > 0:\n",
        "      accuracy_token = sum(epoch_accuracies_token)/len(epoch_accuracies_token)\n",
        "\n",
        "    bleu_score = 0\n",
        "    if len(epoch_bleu_scores) > 0:\n",
        "      bleu_score = sum(epoch_bleu_scores)/len(epoch_bleu_scores)\n",
        "    \n",
        "    bleu_score_punctstrip = 0\n",
        "    if len(epoch_bleu_scores_punctstrip) > 0:\n",
        "      bleu_score_punctstrip = sum(epoch_bleu_scores_punctstrip)/len(epoch_bleu_scores_punctstrip)\n",
        "\n",
        "    #print(\"Loss: {}, TokenAccuracy:{}, BleuScore:{}, BleuScorePunctRemove:{}\".format(round(loss,2), round(accuracy_token,2), round(bleu_score,2),round(bleu_score_punctstrip,2) ))\n",
        "    \n",
        "    return loss, accuracy_token, bleu_score, bleu_score_punctstrip\n",
        "\n",
        "\n",
        "def predict_epoch_qa(epoch, model, optimizer, val_loader, val_encodings, output_filename):\n",
        "    epoch_loss = []\n",
        "    epoch_accuracies_token = []\n",
        "    epoch_bleu_scores = []\n",
        "    epoch_bleu_scores_punctstrip = []\n",
        "    epoch_outputs = []\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    pbar = tqdm_notebook(val_loader, desc=\"Validation batch for epoch-{}\".format(epoch))\n",
        "    for _,batch in enumerate(pbar):\n",
        "        \n",
        "        # we don't need to calculate gradients as we're not training\n",
        "        with torch.no_grad():\n",
        "        \n",
        "            # pull all the tensor batches required for training\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            start_positions = batch['start_token_positions'].to(device)\n",
        "            end_positions = batch['end_token_positions'].to(device)\n",
        "            \n",
        "            \n",
        "            # train model on batch and return outputs (incl. loss)\n",
        "            outputs = model(input_ids, attention_mask=attention_mask,\n",
        "                            start_positions=start_positions,\n",
        "                            end_positions=end_positions)\n",
        "            \n",
        "            # extract loss\n",
        "            loss = outputs[0]\n",
        "\n",
        "            batch_loss = loss.item()\n",
        "            if math.isnan(batch_loss) == False:\n",
        "              epoch_loss.append(batch_loss)\n",
        "            \n",
        "            batch_accuracy_token, batch_bleuscore, batch_bleuscore_punct, batch_outputs = compute_scores_qa(outputs, batch, val_encodings)\n",
        "\n",
        "            epoch_outputs.append(batch_outputs)\n",
        "            epoch_accuracies_token.append(batch_accuracy_token)\n",
        "            epoch_bleu_scores.append(batch_bleuscore)\n",
        "            epoch_bleu_scores_punctstrip.append(batch_bleuscore_punct)\n",
        "\n",
        "            pbar.set_postfix_str(\"Loss: {}, BleuScore:{}\".format(round(batch_loss,2), round(batch_bleuscore,2)))\n",
        "   \n",
        "    loss = 0\n",
        "    if len(epoch_loss) > 0 :\n",
        "      loss = sum(epoch_loss) / len(epoch_loss)\n",
        "    \n",
        "    accuracy_token = 0\n",
        "    if len(epoch_accuracies_token) > 0:\n",
        "      accuracy_token = sum(epoch_accuracies_token)/len(epoch_accuracies_token)\n",
        "\n",
        "    bleu_score = 0\n",
        "    if len(epoch_bleu_scores) > 0:\n",
        "      bleu_score = sum(epoch_bleu_scores)/len(epoch_bleu_scores)\n",
        "    \n",
        "    bleu_score_punctstrip = 0\n",
        "    if len(epoch_bleu_scores_punctstrip) > 0:\n",
        "      bleu_score_punctstrip = sum(epoch_bleu_scores_punctstrip)/len(epoch_bleu_scores_punctstrip)\n",
        "\n",
        "    #print(\"Loss: {}, TokenAccuracy:{}, BleuScore:{}, BleuScorePunctRemove:{}\".format(round(loss,2), round(accuracy_token,2), round(bleu_score,2),round(bleu_score_punctstrip,2) ))\n",
        "\n",
        "    df = pd.DataFrame.from_dict(epoch_outputs)\n",
        "    df = df.apply(pd.Series.explode)\n",
        "    df.to_csv(output_filename)\n",
        "\n",
        "    return loss, accuracy_token, bleu_score, bleu_score_punctstrip\n",
        "\n",
        "\n",
        "def train_qa(model, optimizer, lr_scheduler, train_loader, val_loader, train_encodings, val_encodings, output_filename):\n",
        "    \n",
        "    train_losses = []\n",
        "    train_accuracies_token = []\n",
        "    train_bleuscores = []\n",
        "    train_bleuscores_punctstrip = []\n",
        "\n",
        "    val_losses = []\n",
        "    val_accuracies_token = []\n",
        "    val_bleuscores = []\n",
        "    val_bleuscores_punctstrip = []\n",
        "\n",
        "    epochs = np.arange(QA_EPOCH_MAX)\n",
        "\n",
        "    pbar = tqdm_notebook(range(QA_EPOCH_MAX), desc=\"Epoch\", position = 0)\n",
        "    for epoch in pbar:\n",
        "      \n",
        "      epoch_loss_train, epoch_accuracy_token, epoch_bleu_score_train, epoch_bleu_score_punctstrip = train_epoch_qa(epoch, model, optimizer, lr_scheduler, train_loader, train_encodings)\n",
        "      train_losses.append(epoch_loss_train)\n",
        "      train_accuracies_token.append(epoch_accuracy_token)\n",
        "      train_bleuscores.append(epoch_bleu_score_train)\n",
        "      train_bleuscores_punctstrip.append(epoch_bleu_score_punctstrip)\n",
        "\n",
        "      epoch_loss_val, epoch_accuracy_token, epoch_bleu_score_val, epoch_bleu_score_punctstrip = predict_epoch_qa(epoch, model, optimizer, val_loader, val_encodings, output_filename)\n",
        "      val_losses.append(epoch_loss_val)\n",
        "      val_accuracies_token.append(epoch_accuracy_token)\n",
        "      val_bleuscores.append(epoch_bleu_score_val)\n",
        "      val_bleuscores_punctstrip.append(epoch_bleu_score_punctstrip)\n",
        "     \n",
        "      pbar.set_postfix_str(\"Training loss: {} / BleuScore: {}, Validation Loss: {} / BleuScore: {}\".format(\n",
        "              round(epoch_loss_train,2), round(epoch_bleu_score_train,2), \n",
        "              round(epoch_loss_val,2),  round(epoch_bleu_score_val,2)))\n",
        "\n",
        "    scores = {}\n",
        "    scores['Training Loss'] = round(train_losses[-1], 2)\n",
        "    scores['Training Accuracy Token'] = round(train_accuracies_token[-1], 2)\n",
        "    scores['Training BleuScore'] = round(train_bleuscores[-1], 2)\n",
        "    scores['Training BleuScore Punctuation Strip'] = round(train_bleuscores_punctstrip[-1], 2)\n",
        "\n",
        "    scores['Validation Loss'] = round(val_losses[-1], 2)\n",
        "    scores['Validation Accuracy Token'] = round(val_accuracies_token[-1], 2)\n",
        "    scores['Validation BleuScore'] = round(val_bleuscores[-1], 2)\n",
        "    scores['Validation BleuScore Punctuation Strip'] = round(val_bleuscores_punctstrip[-1], 2)\n",
        "\n",
        "    plt.plot(epochs, train_losses, color = 'blue')\n",
        "    plt.plot(epochs, val_losses, color = 'red')\n",
        "    plt.axhline(y=scores['Training Loss'], linestyle='dotted', color = 'blue')\n",
        "    plt.axhline(y=scores['Validation Loss'], linestyle='dotted', color = 'red')\n",
        "    plt.gca().legend(('Training Loss: {}'.format(scores['Training Loss']),\n",
        "                      'Validation Loss: {}'.format(scores['Validation Loss'])))\n",
        "    plt.title(\"Training-Validation Loss Curve - Question & Answering\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.savefig('epoch_trainval_losses_qa.jpg', bbox_inches = 'tight')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    plt.plot(epochs, train_accuracies_token, color = 'blue')\n",
        "    plt.plot(epochs, val_accuracies_token, color = 'red')\n",
        "    plt.axhline(y=scores['Training Accuracy Token'], linestyle='dotted', color = 'blue')\n",
        "    plt.axhline(y=scores['Validation Accuracy Token'], linestyle='dotted', color = 'red')\n",
        "    plt.gca().legend(('Training Accuracy Token-Span: {}'.format(scores['Training Accuracy Token']),\n",
        "                      'Validation Accuracy Token-Span: {}'.format(scores['Validation Accuracy Token'])))\n",
        "    plt.title(\"Training-Validation Accuracy Token-Span Curve - Question & Answering\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.savefig('epoch_trainval_accuracy_tokenspan_qa.jpg', bbox_inches = 'tight')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    \"\"\"\n",
        "    plt.plot(epochs, train_bleuscores, color = 'blue')\n",
        "    plt.plot(epochs, val_bleuscores, color = 'red')\n",
        "    plt.axhline(y=scores['Training BleuScore'], linestyle='dotted', color = 'blue')\n",
        "    plt.axhline(y=scores['Validation BleuScore'], linestyle='dotted', color = 'red')\n",
        "    plt.gca().legend(('Training BleuScore: {}'.format(scores['Training BleuScore']),\n",
        "                      'Validation BleuScore: {}'.format(scores['Validation BleuScore'])))\n",
        "    plt.title(\"Training-Validation BleuScore Curve - Question & Answering\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"BleuScore\")\n",
        "    #plt.show()\n",
        "    plt.savefig('epoch_trainval_accuracy_bleuscore_qa.jpg', bbox_inches = 'tight')\n",
        "    plt.close()\n",
        "    \"\"\"\n",
        "    \n",
        "    plt.plot(epochs, train_bleuscores_punctstrip, color = 'blue')\n",
        "    plt.plot(epochs, val_bleuscores_punctstrip, color = 'red')\n",
        "    plt.axhline(y=scores['Training BleuScore Punctuation Strip'], linestyle='dotted', color = 'blue')\n",
        "    plt.axhline(y=scores['Validation BleuScore Punctuation Strip'], linestyle='dotted', color = 'red')\n",
        "    plt.gca().legend(('Training BleuScore: {}'.format(scores['Training BleuScore Punctuation Strip']),\n",
        "                      'Validation BleuScore: {}'.format(scores['Validation BleuScore Punctuation Strip'])))\n",
        "    plt.title(\"Training-Validation BleuScore Curve - Question & Answering\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"BleuScore\")\n",
        "    plt.savefig('epoch_trainval_accuracy_bleuscore_punctstrip_qa.jpg', bbox_inches = 'tight')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    \n",
        "    return scores\n",
        "\n",
        "import csv\n",
        "def print_stats(model_name, train_scores):\n",
        "    stats = {}\n",
        "    stats['Model Name'] = model_name\n",
        "    stats['Optimizer'] = QA_OPTIMIZER_NAME\n",
        "    stats['QA_DROPOUT_HIDDEN'] = QA_DROPOUT_HIDDEN\n",
        "    stats['QA_DROPOUT_ATTENTION'] = QA_DROPOUT_ATTENTION\n",
        "    stats['Learning Rate'] = QA_LEARNING_RATE\n",
        "    stats['Weight Decay'] = QA_WEIGHT_DECAY\n",
        "    stats['TOP_N_ROUGE_SCORES'] = TOP_N_ROUGE_SCORES\n",
        "    stats['REMOVE_PUNCTUATIONS'] = REMOVE_PUNCTUATIONS\n",
        "    stats['epoch'] = QA_EPOCH_MAX\n",
        "    \n",
        "    stats.update(train_scores)\n",
        "\n",
        "    # write model parameters to file\n",
        "    with open('score_qa.csv', 'w') as csvfile:  \n",
        "        writer = csv.writer(csvfile)\n",
        "        for key, value in stats.items():\n",
        "           writer.writerow([key, value])\n",
        "\n",
        "    # Append model parameters to file for iterative experiments\n",
        "    with open('scores_qa.csv', 'a', encoding='utf8', newline='') as csvfile:\n",
        "        fc = csv.DictWriter(csvfile, fieldnames=stats.keys(),)\n",
        "\n",
        "        if csvfile.tell() == 0:\n",
        "            fc.writeheader()\n",
        "\n",
        "        fc.writerow(stats)\n",
        "  \n",
        "'''\n",
        "    Block for Main Loop\n",
        "'''\n",
        "\n",
        "def process_qa_document(train_df, val_df):\n",
        "  scores_qa = []\n",
        "\n",
        "  pbar = tqdm_notebook(range(len(QA_MODELS)), desc=\"QA-Document Training using Huggingface Model\")\n",
        "  for i in pbar:\n",
        "    scores = {}\n",
        "    \n",
        "    qa_model_name = QA_MODELS[i][\"name\"]\n",
        "    pbar.set_postfix_str(qa_model_name)\n",
        "\n",
        "    try:\n",
        "      tokenizer = AutoTokenizer.from_pretrained(qa_model_name)\n",
        "\n",
        "      train_qad_encodings, train_qad_loader = prepare_data_qa_document(tokenizer, train_df, QA_TRAIN_BATCH_SIZE)\n",
        "      val_qad_encodings, val_qad_loader = prepare_data_qa_document(tokenizer, val_df, QA_VAL_BATCH_SIZE)\n",
        "\n",
        "      # create mode\n",
        "      model, optimizer, lr_scheduler = create_model_qa(qa_model_name, len(train_qad_loader))\n",
        "\n",
        "      #train\n",
        "      scores = train_qa(model, optimizer, lr_scheduler, train_qad_loader, val_qad_loader, train_qad_encodings, val_qad_encodings, 'output_qa_document.csv')\n",
        "\n",
        "      print(\"Question Answering Document - Statistics\")\n",
        "      print(scores)\n",
        "      print_stats(qa_model_name, scores)\n",
        "      print(\"--------------------------------------------------------------------\")\n",
        "\n",
        "      model_path = 'models/{}-doc-cbw22-djoshi'.format(qa_model_name)\n",
        "      model.save_pretrained(model_path)\n",
        "      tokenizer.save_pretrained(model_path)\n",
        "    except Exception as ex:\n",
        "      scores[\"Error\"] = '{}'.format(ex)\n",
        "      print(scores[\"Error\"])\n",
        "\n",
        "    scores_qa.append(scores)\n",
        "\n",
        "    return scores\n",
        "\n",
        "def process_qa_statement(train_df, val_df, se_val_outputs_df):\n",
        "  scores_qa = []\n",
        "\n",
        "  pbar = tqdm_notebook(range(len(QA_MODELS)), desc=\"QA-Statement Training using Huggingface Model\")\n",
        "  for i in pbar:\n",
        "    scores = {}\n",
        "    \n",
        "    qa_model_name = QA_MODELS[i][\"name\"]\n",
        "    pbar.set_postfix_str(qa_model_name)\n",
        "\n",
        "    try:\n",
        "      tokenizer = AutoTokenizer.from_pretrained(qa_model_name)\n",
        "\n",
        "      train_qad_encodings, train_qad_loader = prepare_data_qa_statement(tokenizer, train_df, QA_TRAIN_BATCH_SIZE)\n",
        "      val_qad_encodings, val_qad_loader = prepare_data_qa_statement(tokenizer, val_df, QA_VAL_BATCH_SIZE)\n",
        "\n",
        "      # create mode\n",
        "      model, optimizer, lr_scheduler = create_model_qa(qa_model_name, len(train_qad_loader))\n",
        "\n",
        "      #train\n",
        "      scores = train_qa(model, optimizer, lr_scheduler, train_qad_loader, val_qad_loader, train_qad_encodings, val_qad_encodings, 'output_qa_statement.csv')\n",
        "\n",
        "      print(\"Question Answering Statement - Statistics\")\n",
        "      print(scores)\n",
        "      print_stats(qa_model_name, scores)\n",
        "      print(\"--------------------------------------------------------------------\")\n",
        "\n",
        "      model_path = 'models/{}-statement-cbw22-djoshi'.format(qa_model_name)\n",
        "      model.save_pretrained(model_path)\n",
        "      tokenizer.save_pretrained(model_path)\n",
        "\n",
        "      #predict SE output\n",
        "      if se_val_outputs_df is not None:\n",
        "        print(\"Predicting Spoiler Spans for spoiler statements predicted by BertSentenceClass Extractor...\")\n",
        "        se_val_outputs_encodings, se_outputs_loader = get_dataloader_qa_statement(tokenizer, se_val_outputs_df, QA_VAL_BATCH_SIZE)\n",
        "        loss, accuracy_token, bleu_score, bleu_score_punctstrip = predict_epoch_qa(-1, model, optimizer, se_outputs_loader, se_val_outputs_encodings, 'output_se_output_qa_statement.csv')\n",
        "        print(\"Loss: {}, TokenAccuracy:{}, BleuScore:{}, BleuScorePunctRemove:{}\".format(round(loss,2), round(accuracy_token,2), round(bleu_score,2),round(bleu_score_punctstrip,2) ))\n",
        "    except Exception as ex:\n",
        "      scores[\"Error\"] = '{}'.format(ex)\n",
        "      print(scores[\"Error\"])\n",
        "\n",
        "    scores_qa.append(scores)\n",
        "\n",
        "    return scores\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 601,
          "referenced_widgets": [
            "59f6dcbc24b848a28de55d4b37bfa1a3",
            "268c3e8c498745f99ac243c5854e8373",
            "3663c153d36a46f683873b6e7fb6f4d0",
            "a51c2efa0cca4866b0488d540ee6db27",
            "9880b677a7d145509f23e73d07567cb8",
            "77f0b1232c7d48bd89e3c3afa26b05af",
            "94b701db1d014bf09c01cf011ec311b8",
            "3e45666aee0745669bff9ae96a705d55",
            "b0f352fd29ec444d8048449221cb6a39",
            "cd41aaec2c5c486d840badc50e10c0aa",
            "b42bce06ea21486aa1ad1ad4bbcb1114",
            "3349fa8a10c24b33a1d7a26c67537a6f",
            "51f5a470863f4049b989de4c05209c18",
            "2150d4f44e0b486abe4462b12a224cc3",
            "e818f13944314b748c809802c9062ee4",
            "b5833430a13b45cab902e20e623b9bf7",
            "5a13f3de7eda4d548371a987c5fd1666",
            "1407c35080a748b0b342a5c2404b726b",
            "51fb321697f643acacc2f9bb33b01efb",
            "2e9c8f6d0ee04014a0f3ba5d8d12ea42",
            "13d7c8d6a97a4d6b9338ae4720e9fa76",
            "b79c653bdf634dd9ae8094815d823daa",
            "b60a21b611574589bae85f9c71145363",
            "40a09cee499a4ab881350096d6cca370",
            "dff00870fba44f6997d381e882db4570",
            "6c377b1564cb4ade8b36e6fedcccaf68",
            "3f39a67614c749efaec98bfe76d1e8b0",
            "063f8d7e0676428783c6cfb678152d51",
            "93e73ac794b34273a274d7711d35fb2d",
            "9eccbccf14184e969cf5413ca8527f8a",
            "2d3d974f61bf42d7867530476616fe82",
            "73c9816079484ff9809718423553ee59",
            "29775d5d51a54bc0bda575f6a6ccdc65",
            "cbb6ef9be1cd479aa577a91d1d6bd6d4",
            "62d7774294c74a4fbb1456d5c750e02d",
            "aca731e245ea4f4082e9e93676c3d097",
            "b6c876da6304473db29c1d2263bfe171",
            "9199b0d3355243a78bed8ec797515876",
            "0d89839a7c7146c59bc24bd474347ba3",
            "45cea92c513943ac97359898b9aa9a69",
            "8fee1d05ecc2439c9d0226bd741bc4f4",
            "80852babc5794fbe9e1b82dc1f67da3c",
            "8e8e2d4c4f2f4a85a34d9fb9cda433c6",
            "5f4a12cd2b4c44cba96cfa518c89a2bc",
            "3d0a2134cddd496783edfd59a8cf6639",
            "39dd3f44e2bd47e68ce83612d6b86a9c",
            "b183bf0d427c45c1885fb77d1cdb3ba4",
            "7fc0b98da519449ba7d41c51d829248e",
            "03eb5cb4507043ff9153c74b9d69643d",
            "c7c8e916e3f34b36b027a0afeaab4aa3",
            "cac991611dc740bcab8629397fd7b7ab",
            "83d3d58b88c44d6cb840f4a75b8df5bc",
            "3388144a0a7e4700b1fccce589525017",
            "47ad1b2443e74395adf0aec88ea5dfb5",
            "e2bf264659bf40b9b1912217f1b8009b",
            "070380841f904c039de1dc31a88156e7",
            "1872b5725eea4d2c90e372becc4e32a4",
            "009296815d144a338cb686e41485f044",
            "1d3a9d7c79a24168babd937ff08fb0a5",
            "acc5336e1ac64de3baf59db72fc50bf2",
            "f92ff6788a484a3ebe5b147666fef716",
            "3932ca4d496743cf83690db37600a7ae",
            "38970bff8f744291906f08172b0377ed",
            "0dda187b01e64d509305f7515de6c343",
            "7fd82f0f6c1245b198223bc8fcebcb97",
            "a061b0c2d6ff4bf7accd1951b47ce96c"
          ]
        },
        "id": "wsbesy3s6Yds",
        "outputId": "3f5c6a55-c3bf-4d50-b78d-a6d1a0325cd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device = cuda\n",
            "Loading Training and Validation Data...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "QA-Document Training using Huggingface Model:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "59f6dcbc24b848a28de55d4b37bfa1a3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3349fa8a10c24b33a1d7a26c67537a6f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b60a21b611574589bae85f9c71145363"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cbb6ef9be1cd479aa577a91d1d6bd6d4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d0a2134cddd496783edfd59a8cf6639"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/501M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "070380841f904c039de1dc31a88156e7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-1f3df059e3bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mPERFORM_DOCUMENT_QA\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0mset_qa_document_configuration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m   \u001b[0mprocess_qa_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_phrase_passage_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_phrase_passage_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mPERFORM_SENTENCE_EXTRACTION\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-f45805870422>\u001b[0m in \u001b[0;36mprocess_qa_document\u001b[0;34m(train_df, val_df)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m       \u001b[0;31m# create mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m       \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model_qa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqa_model_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_qad_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m       \u001b[0;31m#train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-f45805870422>\u001b[0m in \u001b[0;36mcreate_model_qa\u001b[0;34m(model_name, dl_len)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mconfiguration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQA_BERT_NUM_HIDDEN_LAYERS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mconfiguration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_attention_heads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQA_BERT_NUM_ATTENTION_HEADS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForQuestionAnswering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfiguration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    464\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2135\u001b[0m                         \u001b[0m_commit_hash\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcommit_hash\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2136\u001b[0m                     )\n\u001b[0;32m-> 2137\u001b[0;31m                     \u001b[0mresolved_archive_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcached_file_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2139\u001b[0m                     \u001b[0;31m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    410\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m             )\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1240\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"downloading %s to %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1242\u001b[0;31m             http_get(\n\u001b[0m\u001b[1;32m   1243\u001b[0m                 \u001b[0murl_to_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m                 \u001b[0mtemp_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries)\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetEffectiveLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOTSET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     )\n\u001b[0;32m--> 495\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# filter out keep-alive new chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stream'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Platform-specific: Buggy versions of Python.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m                     \u001b[0;31m# Close the connection when no data is returned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1241\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "'''\n",
        "    Block for Main Loop\n",
        "'''\n",
        "import os\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from transformers import logging\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "def seed_torch(seed=1029):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
        "\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "def load_device():\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "    elif torch.backends.mps.is_available():\n",
        "        device = \"mps\"\n",
        "    device = torch.device(device)\n",
        "    print(\"Device = {}\".format(device))    \n",
        "    return device\n",
        "\n",
        "\n",
        "\n",
        "seed_torch()\n",
        "device = load_device()\n",
        "\n",
        "# load json files in to pandas dataframe and curate\n",
        "print(\"Loading Training and Validation Data...\")\n",
        "train_df, train_phrase_passage_df, train_multi_df = load_data('train.jsonl')\n",
        "val_df, val_phrase_passage_df, val_multi_df = load_data('validation.jsonl')\n",
        "\n",
        "se_val_outputs_df = None\n",
        "if PERFORM_DOCUMENT_QA == True:\n",
        "  set_qa_document_configuration()\n",
        "  process_qa_document(train_phrase_passage_df, val_phrase_passage_df)\n",
        "\n",
        "if PERFORM_SENTENCE_EXTRACTION == True:\n",
        "  set_se_configuration()\n",
        "  scores_se, se_val_outputs_df = process_se(train_df, val_df)\n",
        "  if se_val_outputs_df is not None:\n",
        "    append_info_to_se_output(val_df, se_val_outputs_df)\n",
        "    se_val_outputs_df.to_csv(\"se_val_outputs_df.csv\")\n",
        "\n",
        "if PERFORM_SENTENCE_QA == True:\n",
        "  set_qa_statement_configuration()\n",
        "  process_qa_statement(train_phrase_passage_df, val_phrase_passage_df, se_val_outputs_df)\n",
        "\n",
        "  \n",
        "\n",
        "print(\"Finished Processing\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "59f6dcbc24b848a28de55d4b37bfa1a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_268c3e8c498745f99ac243c5854e8373",
              "IPY_MODEL_3663c153d36a46f683873b6e7fb6f4d0",
              "IPY_MODEL_a51c2efa0cca4866b0488d540ee6db27"
            ],
            "layout": "IPY_MODEL_9880b677a7d145509f23e73d07567cb8"
          }
        },
        "268c3e8c498745f99ac243c5854e8373": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77f0b1232c7d48bd89e3c3afa26b05af",
            "placeholder": "​",
            "style": "IPY_MODEL_94b701db1d014bf09c01cf011ec311b8",
            "value": "QA-Document Training using Huggingface Model:   0%"
          }
        },
        "3663c153d36a46f683873b6e7fb6f4d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e45666aee0745669bff9ae96a705d55",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b0f352fd29ec444d8048449221cb6a39",
            "value": 0
          }
        },
        "a51c2efa0cca4866b0488d540ee6db27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd41aaec2c5c486d840badc50e10c0aa",
            "placeholder": "​",
            "style": "IPY_MODEL_b42bce06ea21486aa1ad1ad4bbcb1114",
            "value": " 0/1 [00:08&lt;?, ?it/s, roberta-base]"
          }
        },
        "9880b677a7d145509f23e73d07567cb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77f0b1232c7d48bd89e3c3afa26b05af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94b701db1d014bf09c01cf011ec311b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e45666aee0745669bff9ae96a705d55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0f352fd29ec444d8048449221cb6a39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cd41aaec2c5c486d840badc50e10c0aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b42bce06ea21486aa1ad1ad4bbcb1114": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3349fa8a10c24b33a1d7a26c67537a6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_51f5a470863f4049b989de4c05209c18",
              "IPY_MODEL_2150d4f44e0b486abe4462b12a224cc3",
              "IPY_MODEL_e818f13944314b748c809802c9062ee4"
            ],
            "layout": "IPY_MODEL_b5833430a13b45cab902e20e623b9bf7"
          }
        },
        "51f5a470863f4049b989de4c05209c18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a13f3de7eda4d548371a987c5fd1666",
            "placeholder": "​",
            "style": "IPY_MODEL_1407c35080a748b0b342a5c2404b726b",
            "value": "Downloading: 100%"
          }
        },
        "2150d4f44e0b486abe4462b12a224cc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51fb321697f643acacc2f9bb33b01efb",
            "max": 481,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2e9c8f6d0ee04014a0f3ba5d8d12ea42",
            "value": 481
          }
        },
        "e818f13944314b748c809802c9062ee4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13d7c8d6a97a4d6b9338ae4720e9fa76",
            "placeholder": "​",
            "style": "IPY_MODEL_b79c653bdf634dd9ae8094815d823daa",
            "value": " 481/481 [00:00&lt;00:00, 20.4kB/s]"
          }
        },
        "b5833430a13b45cab902e20e623b9bf7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a13f3de7eda4d548371a987c5fd1666": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1407c35080a748b0b342a5c2404b726b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51fb321697f643acacc2f9bb33b01efb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e9c8f6d0ee04014a0f3ba5d8d12ea42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "13d7c8d6a97a4d6b9338ae4720e9fa76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b79c653bdf634dd9ae8094815d823daa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b60a21b611574589bae85f9c71145363": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_40a09cee499a4ab881350096d6cca370",
              "IPY_MODEL_dff00870fba44f6997d381e882db4570",
              "IPY_MODEL_6c377b1564cb4ade8b36e6fedcccaf68"
            ],
            "layout": "IPY_MODEL_3f39a67614c749efaec98bfe76d1e8b0"
          }
        },
        "40a09cee499a4ab881350096d6cca370": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_063f8d7e0676428783c6cfb678152d51",
            "placeholder": "​",
            "style": "IPY_MODEL_93e73ac794b34273a274d7711d35fb2d",
            "value": "Downloading: 100%"
          }
        },
        "dff00870fba44f6997d381e882db4570": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9eccbccf14184e969cf5413ca8527f8a",
            "max": 898823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2d3d974f61bf42d7867530476616fe82",
            "value": 898823
          }
        },
        "6c377b1564cb4ade8b36e6fedcccaf68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73c9816079484ff9809718423553ee59",
            "placeholder": "​",
            "style": "IPY_MODEL_29775d5d51a54bc0bda575f6a6ccdc65",
            "value": " 899k/899k [00:00&lt;00:00, 2.85MB/s]"
          }
        },
        "3f39a67614c749efaec98bfe76d1e8b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "063f8d7e0676428783c6cfb678152d51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93e73ac794b34273a274d7711d35fb2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9eccbccf14184e969cf5413ca8527f8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d3d974f61bf42d7867530476616fe82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "73c9816079484ff9809718423553ee59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29775d5d51a54bc0bda575f6a6ccdc65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cbb6ef9be1cd479aa577a91d1d6bd6d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_62d7774294c74a4fbb1456d5c750e02d",
              "IPY_MODEL_aca731e245ea4f4082e9e93676c3d097",
              "IPY_MODEL_b6c876da6304473db29c1d2263bfe171"
            ],
            "layout": "IPY_MODEL_9199b0d3355243a78bed8ec797515876"
          }
        },
        "62d7774294c74a4fbb1456d5c750e02d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d89839a7c7146c59bc24bd474347ba3",
            "placeholder": "​",
            "style": "IPY_MODEL_45cea92c513943ac97359898b9aa9a69",
            "value": "Downloading: 100%"
          }
        },
        "aca731e245ea4f4082e9e93676c3d097": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fee1d05ecc2439c9d0226bd741bc4f4",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_80852babc5794fbe9e1b82dc1f67da3c",
            "value": 456318
          }
        },
        "b6c876da6304473db29c1d2263bfe171": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e8e2d4c4f2f4a85a34d9fb9cda433c6",
            "placeholder": "​",
            "style": "IPY_MODEL_5f4a12cd2b4c44cba96cfa518c89a2bc",
            "value": " 456k/456k [00:00&lt;00:00, 1.95MB/s]"
          }
        },
        "9199b0d3355243a78bed8ec797515876": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d89839a7c7146c59bc24bd474347ba3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45cea92c513943ac97359898b9aa9a69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8fee1d05ecc2439c9d0226bd741bc4f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80852babc5794fbe9e1b82dc1f67da3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8e8e2d4c4f2f4a85a34d9fb9cda433c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f4a12cd2b4c44cba96cfa518c89a2bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d0a2134cddd496783edfd59a8cf6639": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_39dd3f44e2bd47e68ce83612d6b86a9c",
              "IPY_MODEL_b183bf0d427c45c1885fb77d1cdb3ba4",
              "IPY_MODEL_7fc0b98da519449ba7d41c51d829248e"
            ],
            "layout": "IPY_MODEL_03eb5cb4507043ff9153c74b9d69643d"
          }
        },
        "39dd3f44e2bd47e68ce83612d6b86a9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7c8e916e3f34b36b027a0afeaab4aa3",
            "placeholder": "​",
            "style": "IPY_MODEL_cac991611dc740bcab8629397fd7b7ab",
            "value": "Downloading: 100%"
          }
        },
        "b183bf0d427c45c1885fb77d1cdb3ba4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83d3d58b88c44d6cb840f4a75b8df5bc",
            "max": 1355863,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3388144a0a7e4700b1fccce589525017",
            "value": 1355863
          }
        },
        "7fc0b98da519449ba7d41c51d829248e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47ad1b2443e74395adf0aec88ea5dfb5",
            "placeholder": "​",
            "style": "IPY_MODEL_e2bf264659bf40b9b1912217f1b8009b",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 2.89MB/s]"
          }
        },
        "03eb5cb4507043ff9153c74b9d69643d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7c8e916e3f34b36b027a0afeaab4aa3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cac991611dc740bcab8629397fd7b7ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "83d3d58b88c44d6cb840f4a75b8df5bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3388144a0a7e4700b1fccce589525017": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "47ad1b2443e74395adf0aec88ea5dfb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2bf264659bf40b9b1912217f1b8009b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "070380841f904c039de1dc31a88156e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1872b5725eea4d2c90e372becc4e32a4",
              "IPY_MODEL_009296815d144a338cb686e41485f044",
              "IPY_MODEL_1d3a9d7c79a24168babd937ff08fb0a5"
            ],
            "layout": "IPY_MODEL_acc5336e1ac64de3baf59db72fc50bf2"
          }
        },
        "1872b5725eea4d2c90e372becc4e32a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f92ff6788a484a3ebe5b147666fef716",
            "placeholder": "​",
            "style": "IPY_MODEL_3932ca4d496743cf83690db37600a7ae",
            "value": "Downloading:  17%"
          }
        },
        "009296815d144a338cb686e41485f044": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38970bff8f744291906f08172b0377ed",
            "max": 501200538,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0dda187b01e64d509305f7515de6c343",
            "value": 86968320
          }
        },
        "1d3a9d7c79a24168babd937ff08fb0a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7fd82f0f6c1245b198223bc8fcebcb97",
            "placeholder": "​",
            "style": "IPY_MODEL_a061b0c2d6ff4bf7accd1951b47ce96c",
            "value": " 87.0M/501M [00:02&lt;00:10, 41.0MB/s]"
          }
        },
        "acc5336e1ac64de3baf59db72fc50bf2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f92ff6788a484a3ebe5b147666fef716": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3932ca4d496743cf83690db37600a7ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38970bff8f744291906f08172b0377ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0dda187b01e64d509305f7515de6c343": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7fd82f0f6c1245b198223bc8fcebcb97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a061b0c2d6ff4bf7accd1951b47ce96c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}